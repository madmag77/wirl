import os
import asyncio
import logging
import smtplib
from datetime import datetime, timedelta, timezone
from email.message import EmailMessage
from enum import Enum
from urllib.parse import urljoin

import feedparser
import requests
from aiogram import Bot
from bs4 import BeautifulSoup
from langchain_ollama import ChatOllama
from pydantic import BaseModel, Field
import dotenv

logger = logging.getLogger(__name__)

dotenv.load_dotenv()


class ResourceType(str, Enum):
    RSS = "rss"
    WEB = "web"


class NewsResource(BaseModel):
    url: str = Field(description="Resource URL")
    type: ResourceType = Field(description="Type of the resource")


class NewsItem(BaseModel):
    title: str = Field(description="Title of the news item")
    link: str = Field(description="Link to the news item")
    published: datetime = Field(description="Publication datetime")
    summary: str = Field(description="Summary or excerpt of the item", default="")
    llm_summary: str = Field(description="Summary of the item generated by the LLM", default="")

class NewsItemLLM(BaseModel):
    title: str = Field(description="Title of the news item")
    link: str = Field(description="Link to the news item. Don't change it from the original link.")
    published: datetime = Field(description="Publication datetime")

class NewsItems(BaseModel):
    news_items: list[NewsItemLLM] = Field(description="List of news items")


def get_resources(trigger: str, config: dict) -> dict:
    resources = [
        {"url": "https://karpathy.bearblog.dev/feed/", "type": ResourceType.RSS},
        {"url": "https://www.anthropic.com/news", "type": ResourceType.WEB},
        {"url": "https://openai.com/news/research/", "type": ResourceType.WEB},
    ]
    return {"resources": [NewsResource(**r) for r in resources]}


def fetch_news(resources: list[NewsResource], config: dict) -> dict:
    days_back = config.get("days_back", 7)
    model = config.get("model")
    reasoning = config.get("reasoning", False)
    temperature = config.get("temperature", 0)
    start_date = datetime.utcnow() - timedelta(days=days_back)
    start_date = start_date.replace(tzinfo=timezone.utc)
    end_date = datetime.utcnow() + timedelta(days=1)
    end_date = end_date.replace(tzinfo=timezone.utc)
    news_items: list[NewsItem] = []
    headers = {"User-Agent": "Mozilla/5.0"}

    for res in resources:
        try:
            if res.type == ResourceType.RSS:
                feed = feedparser.parse(res.url)
                for entry in getattr(feed, "entries", []):
                    published_parsed = getattr(entry, "published_parsed", None) or getattr(entry, "updated_parsed", None)
                    if not published_parsed:
                        continue
                    published = datetime(*published_parsed[:6])
                    published = published.replace(tzinfo=timezone.utc)
                    if published < start_date:
                        continue
                    summary = entry.get("summary", "")
                    news_items.append(
                        NewsItem(
                            title=entry.get("title", ""),
                            link=entry.get("link", ""),
                            published=published,
                            summary=summary,
                        )
                    )
            elif res.type == ResourceType.WEB:
                resp = requests.get(res.url, headers=headers, timeout=10)
                if resp.status_code != 200:
                    continue
                soup = BeautifulSoup(resp.text, "html.parser")
                batch_size = 20
                llm = ChatOllama(
                        model=model,
                        temperature=temperature,
                        validate_model_on_init = True,
                        reasoning=reasoning,
                )
                llm_news_item = llm.with_structured_output(NewsItems, method="json_schema")
                for i in range(0, len(soup.find_all("a")), batch_size):
                    text = "\n\n".join(f"link: {a.get('href', '')}, text: {a.get_text(" ", strip=True)}" for a in soup.find_all("a")[i:i+batch_size])
                    resp = llm_news_item.invoke(f"Extract news items from the following text {text}")
                    for item in resp.news_items:
                        # Filter out old news and news with mistakenly parsed published date
                        if item.published < start_date or item.published > end_date:
                            continue

                        link = urljoin(res.url, item.link)
                        summary = ""
                        try:
                            article = requests.get(link, headers=headers, timeout=10)
                            art_soup = BeautifulSoup(article.text, "html.parser")
                            summary = " ".join(art_soup.get_text(" ", strip=True).split())
                        except Exception:
                            pass
                        news_items.append(NewsItem(title=item.title, link=link, published=item.published, summary=summary))
        except Exception as e:
            logger.warning(f"Failed to parse {res.url}: {e}")

    return {"news_items": news_items}


def summarize_news(news_items: list[NewsItem], config: dict) -> dict:
    if not news_items:
        return {"summary": "No new items."}
    
    model = config.get("model")
    reasoning = config.get("reasoning", False)
    temperature = config.get("temperature", 0)
    
    llm = ChatOllama(
        model=model,
        reasoning=reasoning,
        temperature=temperature,
        validate_model_on_init = True,
    )
    
    for item in news_items:
        response = llm.invoke(f"Provide a summary of the following news item:\n{item}")
        summary = getattr(response, "content", str(response))
        item.llm_summary = summary

    final_summary = "\n\n".join([f"{item.title}, {item.link}, {item.published}: {item.llm_summary}" for item in news_items])
    print(final_summary)
    return {"summary": final_summary}


def send_summary(summary: str, config: dict) -> dict:
    delivery_type = config.get("type", "email")
    recipient = config.get("recipient")
    if delivery_type == "telegram":
        token = os.environ.get("TELEGRAM_BOT_TOKEN")
        if not token:
            raise ValueError("TELEGRAM_BOT_TOKEN environment variable is required")
        if not recipient:
            raise ValueError("recipient is required for telegram delivery")

        async def _send() -> None:
            bot = Bot(token=token)
            await bot.send_message(recipient, "Here is the news digest:\n\n" + summary)
            await bot.session.close()

        asyncio.run(_send())
    else:
        smtp_server = os.environ.get("SMTP_SERVER")
        smtp_port = int(os.environ.get("SMTP_PORT", "587"))
        smtp_username = os.environ.get("SMTP_USERNAME")
        smtp_password = os.environ.get("SMTP_PASSWORD")
        from_email = os.environ.get("FROM_EMAIL")

        if not smtp_server:
            raise ValueError("SMTP_SERVER environment variable is required")
        if not from_email:
            raise ValueError("FROM_EMAIL environment variable is required")
        if not recipient:
            raise ValueError("recipient is required for email delivery")

        msg = EmailMessage()
        msg["Subject"] = "Weekly News Digest"
        msg["From"] = from_email
        msg["To"] = recipient
        msg.set_content("Here is the news digest:\n\n" + summary)

        try:
            with smtplib.SMTP(smtp_server, smtp_port) as server:
                server.ehlo()
                server.starttls()
                server.ehlo()
                if smtp_username and smtp_password:
                    server.login(smtp_username, smtp_password)
                server.send_message(msg)
            logger.info("Email sent successfully")
        except Exception as e:
            logger.error(f"Failed to send email: {e}")
            raise e

    return {"success": True}

